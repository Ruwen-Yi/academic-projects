# -*- coding: utf-8 -*-
"""a1790940_Assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CiGGvzY-19MPSMbXjc_L8xGVWTf41s7n
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

dataset = pd.read_csv('/content/drive/MyDrive/Assignment-3/mnist.csv', header=None)

y_train = dataset.iloc[:,:1].values
x_train = dataset.iloc[:,1:].values

x_train.shape # (6000, 784)
dataset

"""### Q1 implement PCA"""

# get covariance matrix
def covariance_matrix(X): # X : d x N
    return (1.0/6000) * (X @ X.T)

# get Eigenvalue and Eigenvector
def eig(CM):
    eigvals, eigvecs = np.linalg.eig(CM)
    k = np.argsort(eigvals)[::-1]               # eigenvalue sort from largest to smallest; K stores the index of the corresponding element; k is a vector
    return eigvals[k], eigvecs[:,k]             # sort eigvals with elements in k; sort columns of eigvecs with elements in k

# centralise data
x_mean = np.mean(x_train, axis=0)
x_c = x_train - x_mean
x_c = x_c.T

# get covariance matrix of X_c
x_c_cm = covariance_matrix(x_c)

# get eigen vector
egigvals, eigvecs = eig(x_c_cm)
eigvecs = eigvecs[:,:10]            #Take the first 10 columns

# get P
P = eigvecs.T

# get transformed data Y and centralise it
Y = P @ x_c                         # Y.shape = 10x6000 (kxN)
Y = Y.T
Y_mean = np.mean(Y, axis=0)
Y_c = Y - Y_mean
Y_c = Y_c.T                         # Y_c.shape = 10x6000 (kxN)

# get covariance_matrix of Y_c
Y_cm = covariance_matrix(Y_c)       # Y_cm.shape = 10x10 (kxk)
pd.DataFrame(np.real(np.around(Y_cm, 0)))

"""### Q2 implement k-means"""

def calculate_loss(x, assignment, centers, num_clusters):
    loss_value=[]                                           # initialize list of loss values
    for i in range(x.shape[0]):
        assign = assignment[i]                              # r_i is a one-row matrix, representing the assignment of sample i
        cluster_idx = np.argwhere(assign==1)[0]                         # get cluster index of sample i
        loss_value += [np.linalg.norm(x[i] - centers[cluster_idx])]     # pick the corresponding center and calculate distance
    return sum(loss_value)

def kmeans(x,k):
    num_clusters = k
    # Pick first k samples
    u_k = x[:num_clusters]                            
    r_k = np.zeros(shape=(x.shape[0],num_clusters))       
    rk_old = np.zeros(shape=(x.shape[0],num_clusters))

    loss = []

    while True:
        rk_old = np.copy(r_k)
        for i in range(x.shape[0]):                                     # E-step: update r_k
            r_k[i] = np.zeros(shape=(1,num_clusters))                   # initialize the assignment of sample i
            cluster_idx = np.argmin([np.linalg.norm(x[i] - u) for u in u_k])    # Get the index of the closest center
            r_k[i][cluster_idx] = 1                                             # Updata the assignment of sample i

        for i in range(num_clusters):                 # M-step: update u_k
            r_i = r_k.T[i]                            # Get cluster i (r_i is a vector)
            index_list = r_i==1                       # Get index of samples grouped into cluster i

            u_k[i] = np.mean(x[index_list], axis=0)   # Update the center of cluster i

        loss += [calculate_loss(x, r_k, u_k, num_clusters)]         # Calculate loss values

        # Check convergency
        if np.all(r_k==rk_old):
            break
        
    return r_k, u_k, loss

"""### Q3 plot the loss curve"""

import matplotlib.pyplot as plt
import seaborn as sns
_,_,loss = kmeans(x_train,10)                                   # re-run the entire file (Ctrl+F9) if the curve is flat
sns.lineplot(x = range(1,len(loss)+1),y=loss, markers='-')
plt.xlabel('iteration times')
plt.ylabel('loss values')
plt.show()

"""### Q4 Find optimal k"""

def sum_loss(x, uk):
    tmp = []
    tmp = [np.min([np.linalg.norm(xi-uj) for uj in uk]) for xi in x]
    return sum(tmp)

loss_list = []
for op_k in range(5,21):
    _,uk,_ = kmeans(x_train[:4000], op_k)
    loss_value = sum_loss(x_train[4000:], uk)
    loss_list += [loss_value]

optimal_k = np.argmin(loss_list) + 5
optimal_k

"""### Q5 kernel k-means"""

# calculate the denominator of rbf
rbf_bottom = 0.
for i in range(500):
    for j in range(500):
        rbf_bottom += (np.linalg.norm(x_train[i] - x_train[j]) ** 2)
rbf_bottom = rbf_bottom/(500**2)

def RBF_kernel(xi,xj):
    top = -(np.linalg.norm(xi - xj) ** 2)
    bottom = rbf_bottom
    return np.exp(top/bottom)

def formKernelMatrix(x):
    k = np.ones(shape=(500,500))
    for i in range(500):
        for j in range(500):
            k[i][j]=RBF_kernel(x[i],x[j])
    return k

# 0. get data and construct kernel matrix
x_train2 = x_train[:500]
kernelMatrix = formKernelMatrix(x_train2)

# 1. randomly pick 5 samples as initial center
center_list = np.random.permutation(500)[:5]
centers = x_train2[center_list]

# 2. initialize assignment
r_k = np.zeros(shape=(500,5))         # Initialize cluster

for i in range(500):
    distances = [((-2) * kernelMatrix[i][c]) for c in center_list]       # Using kernel trick to calculate the distance to mean vectors 
    cluster_idx = np.argmin(distances)                                   # Get the index of the closest mean vector
    r_k[i][cluster_idx] = 1                                              # Update sample assignment

# 3. iteratively assign
while True:
    r_k_old = np.copy(r_k)
    for i in range(500):
        dists=[]                                # initialize distance list
        for k in range(5):
            r_i = (r_k.T[k]/r_k.T[k].sum())     # Get assigment status for sample i, r_i.shape=1x500
            dists += [((r_i @ kernelMatrix) @ r_i.T) - 2 * (r_i @ kernelMatrix[:,i])]   # Calculate with kernel in matrix form

        new_assign = np.argmin([dists])         # Get the index of the smallest distance
        r_k[i] = np.zeros(shape=(1,5))          # initialize the assignment for sample i
        r_k[i][new_assign] = 1                  # Reassign sample i

    if ((r_k_old == r_k).all()):                # Check convergency
        break
